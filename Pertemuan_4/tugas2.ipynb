{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPcZMOgl/21vAfEsSgG+iJp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MaulidinZakaria/16_MaulidinZakaria-Machine_Learning/blob/main/Pertemuan_4/tugas2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1. Import Library**"
      ],
      "metadata": {
        "id": "jL1G5q4WvlYA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "from pathlib import Path\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from zipfile import ZipFile\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler"
      ],
      "metadata": {
        "id": "5usLPVFroAFT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Pathlib.Path: Menyediakan cara berorientasi objek untuk menangani dan memanipulasi jalur file.\n",
        "2. cv2: Library OpenCV yang digunakan untuk pemrosesan gambar, seperti membaca gambar, mengubah ruang warna, dan menghitung histogram.\n",
        "3. numpy (np): Digunakan untuk operasi numerik seperti array dan matriks, penting untuk mengelola data gambar.\n",
        "4. pandas (pd): Untuk manipulasi dan analisis data. Dalam kode ini tidak digunakan secara eksplisit tetapi mungkin diperlukan dalam analisis dataset yang lebih besar.\n",
        "5. ZipFile: Dari modul zipfile di Python, digunakan untuk mengekstrak data dari arsip ZIP, dalam hal ini adalah sekumpulan gambar.\n",
        "6. SVC (Support Vector Classifier): Algoritma Support Vector Machine dari sklearn yang digunakan untuk tugas klasifikasi.\n",
        "7. accuracy_score: Fungsi dari sklearn untuk menghitung akurasi prediksi model.\n",
        "8. GridSearchCV: Digunakan untuk mencari secara menyeluruh nilai hyperparameter terbaik untuk meningkatkan performa model.\n",
        "9. StandardScaler: Untuk menormalkan fitur dengan menghilangkan rata-rata dan menskalakan ke variansi satuan, yang dapat membantu kinerja model SVM."
      ],
      "metadata": {
        "id": "ktT1hjpZBMBY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2. Load Data**"
      ],
      "metadata": {
        "id": "AaaiRUjwvt1I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and extract the dataset from zip file\n",
        "with ZipFile('images.zip', 'r') as zip:\n",
        "    zip.extractall()\n",
        "    print('Dataset extracted!')\n",
        "\n",
        "# Image directories\n",
        "train_dir = \"images/training/\"\n",
        "test_dir = \"images/test/\"\n",
        "\n",
        "# Function to load images and labels\n",
        "def load_dataset(img_dir):\n",
        "    p = Path(img_dir)\n",
        "    dirs = p.glob('*')\n",
        "    img_list = []\n",
        "    for dir in dirs:\n",
        "        label = str(dir).split('/')[-1]\n",
        "        for file in dir.glob('*.jpg'):\n",
        "            img = cv2.imread(str(file))\n",
        "            if img is not None:\n",
        "                img_list.append((img, label))\n",
        "    return img_list\n",
        "\n",
        "# Load the training and test data\n",
        "train_img = load_dataset(train_dir)\n",
        "test_img = load_dataset(test_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RyOGFNk0oAN7",
        "outputId": "2b338c5b-1736-4dbe-8d13-fa0e1ca05f7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset extracted!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Bagian ini mengekstrak dataset dari file ZIP bernama images.zip ke direktori saat ini sehingga data gambar dapat diakses untuk proses selanjutnya.\n",
        "\n",
        "2. Bagian ini mendefinisikan jalur direktori yang berisi gambar-gambar pelatihan (training) dan pengujian (test).\n",
        "\n",
        "3. Fungsi load_dataset memuat dataset gambar dari direktori tertentu, membaca gambar-gambar tersebut, mengasosiasikan gambar dengan label (nama sub-direktori tempat gambar berada), dan mengembalikannya sebagai list berisi tuple (gambar, label).\n"
      ],
      "metadata": {
        "id": "wfwwqW-MGlq_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3. Pra Pengolahan Data**"
      ],
      "metadata": {
        "id": "l_a_lEXCvt6J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to standardize the input image\n",
        "def standarized_input(image, size=(1100, 600)):  # Reduce size to minimize feature complexity\n",
        "    std_img = cv2.resize(image, size)\n",
        "    return std_img\n",
        "\n",
        "# Preprocess the image and label\n",
        "def preprocess(img_list):\n",
        "    std_img_list = []\n",
        "    for item in img_list:\n",
        "        image = standarized_input(item[0])  # Standardize image\n",
        "        label = 1 if item[1] == 'day' else 0  # Encode label\n",
        "        std_img_list.append((image, label))\n",
        "    return std_img_list\n",
        "\n",
        "train_std_img_list = preprocess(train_img)\n",
        "test_std_img_list = preprocess(test_img)"
      ],
      "metadata": {
        "id": "T5rumTPSoAVt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. standarized_input berfungsi untuk menyamakan ukuran gambar agar dataset memiliki dimensi yang konsisten.\n",
        "2. preprocess mengubah ukuran gambar dan menyandikan label gambar ke dalam format numerik, sehingga data siap digunakan untuk pelatihan atau pengujian model machine learning."
      ],
      "metadata": {
        "id": "u5pSeJlaHT-n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4. Ekstrasi Fitur dengan Histogram**"
      ],
      "metadata": {
        "id": "_Alfppdyvt9u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to calculate color histogram for each image using fewer bins to reduce complexity\n",
        "def calc_histogram(image):\n",
        "    # Convert to HSV\n",
        "    hsv_image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
        "\n",
        "    # Calculate histogram for all three channels H, S, and V using fewer bins\n",
        "    hist_h = cv2.calcHist([hsv_image], [0], None, [20], [0, 180])  # Reduce bins for Hue\n",
        "    hist_s = cv2.calcHist([hsv_image], [1], None, [20], [0, 256])  # Reduce bins for Saturation\n",
        "    hist_v = cv2.calcHist([hsv_image], [2], None, [20], [0, 256])  # Reduce bins for Value\n",
        "\n",
        "    # Normalize the histograms\n",
        "    hist_h = cv2.normalize(hist_h, hist_h).flatten()\n",
        "    hist_s = cv2.normalize(hist_s, hist_s).flatten()\n",
        "    hist_v = cv2.normalize(hist_v, hist_v).flatten()\n",
        "\n",
        "    # Concatenate all histograms into one feature vector\n",
        "    hist_features = np.concatenate([hist_h, hist_s, hist_v])\n",
        "\n",
        "    return hist_features\n",
        "\n",
        "# Extract histogram features and labels for the dataset\n",
        "def extract_histogram_features(img_list):\n",
        "    features = []\n",
        "    labels = []\n",
        "    for item in img_list:\n",
        "        hist = calc_histogram(item[0])  # Extract histogram feature\n",
        "        features.append(hist)\n",
        "        labels.append(item[1])  # Label: day=1, night=0\n",
        "    return np.array(features), np.array(labels)\n",
        "\n",
        "# Extract features for train and test set\n",
        "X_train, y_train = extract_histogram_features(train_std_img_list)\n",
        "X_test, y_test = extract_histogram_features(test_std_img_list)"
      ],
      "metadata": {
        "id": "42-xd9maoAfg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Fungsi calc_histogram digunakan untuk menghitung dan mengekstrak histogram warna dari tiap gambar dalam ruang warna HSV, dan hasilnya digunakan sebagai fitur input untuk model machine learning.\n",
        "2. Fungsi extract_histogram_features mengambil daftar gambar dan label, lalu mengekstrak fitur histogram dari gambar tersebut untuk digunakan dalam pelatihan atau pengujian model."
      ],
      "metadata": {
        "id": "dJRaowBkHkvL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **5. Normalisasi Fitur dan Tuning Hyperparameter**"
      ],
      "metadata": {
        "id": "Hydl8jWCvuAs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalize the feature data\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Modify hyperparameter tuning to reduce overfitting\n",
        "param_grid = {\n",
        "    'C': [0.001, 0.01, 0.1],  # Smaller C values to reduce overfitting\n",
        "    'gamma': [0.1, 0.01, 0.001],  # Test smaller gamma to balance generalization\n",
        "    'kernel': ['rbf']  # Using RBF kernel\n",
        "}\n",
        "\n",
        "svc = SVC()\n",
        "\n",
        "# Grid search to find the best hyperparameters\n",
        "grid_search = GridSearchCV(svc, param_grid, cv=5, verbose=1, n_jobs=-1)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Print the best parameters from grid search\n",
        "print(f'Best parameters found: {grid_search.best_params_}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vHFfN3UGoAn1",
        "outputId": "6f0d281c-4aa8-408b-ab61-6df1772e0827"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n",
            "Best parameters found: {'C': 0.1, 'gamma': 0.01, 'kernel': 'rbf'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Menormalkan data agar model SVC bisa bekerja lebih baik dengan data fitur yang terdistribusi secara konsisten.\n",
        "2. Mencari kombinasi hyperparameter terbaik untuk model SVC dengan menggunakan grid search dan cross-validation, sehingga model tidak overfit dan bisa bergeneralisasi dengan baik ke data baru"
      ],
      "metadata": {
        "id": "yCu2cFWHHyDw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **6. Evaluasi Model**"
      ],
      "metadata": {
        "id": "trZFn9prvuDG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model with the best hyperparameters\n",
        "best_model = grid_search.best_estimator_\n",
        "best_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_train_pred = best_model.predict(X_train)\n",
        "y_test_pred = best_model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
        "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
        "\n",
        "# Print evaluation results\n",
        "print(f'Accuracy on train data: {train_accuracy * 100:.2f}%')\n",
        "print(f'Accuracy on test data: {test_accuracy * 100:.2f}%')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_v6zPIkZ571H",
        "outputId": "76020931-e2fc-49ad-88b6-369359a5d5b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on train data: 98.75%\n",
            "Accuracy on test data: 96.25%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Pelatihan model dilakukan dengan hyperparameter terbaik yang dihasilkan dari grid search.\n",
        "2. Model melakukan prediksi pada data latih dan data uji.\n",
        "3. Akurasi pada kedua dataset dihitung dan ditampilkan, yang memberikan gambaran tentang seberapa baik model bekerja di kedua situasi tersebut: pelatihan (menghafal pola data) dan pengujian (kemampuan generalisasi ke data baru)."
      ],
      "metadata": {
        "id": "jvw1QvjPIEE1"
      }
    }
  ]
}